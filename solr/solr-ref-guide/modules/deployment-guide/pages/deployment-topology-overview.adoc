= Deployment Topology Overview
// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

////
This page has a number of graphs to help you visualize different Solr deployment strategies.

The graphs are developed using Mermaid syntax.

The site https://magjac.com/graphviz-visual-editor/ allows you to play with those graphs in real time.
////

This section embodies the Solr community's thoughts on best practices for deploying Solr depending on your needs.

== Overview
There is a logical progression of topologies for scaling Solr based on the number of nodes you anticipate needing.  

[graphviz]
....
graph {
  node [style=rounded, shape=circle, fixedsize=true, width=2, height=2]
  //layout=circo
  overlap=false
  rankdir=LR;  // Set the direction to left to right
  node1 [label="1. Single \nNode"]
  node2 [label="2. Two Nodes"]
  node3 [label="3. Three or Five Nodes"]
  node4 [label="4. Six to Twelve\nNodes"]
  node5 [label="5. Twelve to \nTwenty Five\nNodes"]
  node6 [label="6. Twenty Six \nand Beyond"]

  node1 -- node2
  node2 -- node3
  node3 -- node4
  node4 -- node5
  node5 -- node6  
}
....


== Solr from smallest to largest

When we start up Solr on our computer, we're already starting Solr with the underpinnings required to let Solr scale in a smooth fashion, the coordination library ZooKeeper.
ZooKeeper is the unifying technology that supports maintaining state from a single node up to many 1000's of nodes.

=== Simplest Setup

If you only need a single Solr node, then it's perfectly reasonable to start Solr with `bin/solr start`.   You will have a single Solr node running in SolrCloud mode, with all the lovely APIs and features that SolrCloud provides.

[graphviz]
....
digraph single_node {
  node [style=rounded]
  node1 [shape=box, fillcolor=yellow]
  
  node1
}
....

Use this approach when:

* You are just playing around
* You aren't worried about HA or Failover
* You want the simplest deployment approach.


=== Introducing Fail Over

The next most common setup after a single node is having two separate nodes running on separate machines, with one as the xref:cluster-types.adoc#leaders[Leader] and the other as the Follower.  

NOTE: This needs Jason's https://github.com/apache/solr/pull/2391 to get to done done and be able to recommend this!  Or, you start one solr with zk and the other connects.  that is today.

This leverages a simplistic ZooKeeper xref:getting-started:solr-glossary.adoc#ensemble[Ensemble] setup.   You can start a pair of Solrs and have their embedded ZooKeeper join each other to form an Ensemble.   And yes, I hear you when you say "this isn't an odd number and ZK quorums should be an odd number to avoid split brain etc."

[graphviz]
....
graph simple_embedded_zk_ensemble {
  node [style=rounded]
  layout=neato
  node1 [shape=box]
  node2 [shape=box]
  
  node1 -- node2
  node2 -- node1
}
....


Use this approach when:

* You have only two Solr nodes and they are close to each other in network terms.
* This approach is for when you want failover, but you aren't worried about high availability. You have a load balancer in front of the two Solr nodes that detects when one goes down and redirects traffic to the remaining node for querying.
* You are prepared to handle the consequences for indexing operations if one of the nodes goes down.

=== Moving to High Availability

NOTE: What is the difference between failover and high availability? Failover, in this two-node situation, means that you can still issue queries and keep your application running, but you can no longer index data because ZooKeeper can't form a quorum. This is often referred to as a Split Brain situation. High Availability means that your service continues uninterrupted in the event of any node going down.

You can then scale beyond two nodes by running either three or five Solr nodes:

NOTE: We specify three or five Solr nodes because ZooKeeper is running on each node, and requires an odd number of nodes to prevent Split Brain issues.

[graphviz]
....
graph simple_embedded_zk_ensemble {
  node [style=rounded]
  layout=neato
  node1 [shape=box]
  node2 [shape=box]
  node3 [shape=box]
  node4 [shape=box]
  node5 [shape=box]
  
  node1 -- node2
  node2 -- node3
  node3 -- node4
  node4 -- node5
  node5 -- node1
}
....

Use these approaches when:

* You want to be able to split your logical Collection across multiple Shards and distribute Replicas around the cluster.
* You don't want to go through the effort of deploying a separate ZK ensemble independently. And honestly, you don't need to either.


Some cons to this approach are:

* Having five ZooKeeper instances all updating each other is fine, but it starts to break down if you expand to 9 or 11 ZooKeeper instances forming the Quorum.
* We currently don't have any flexible resizing of the quorum. You need to select the appropriate size when setting up your cluster.

=== Moving Beyond the Basic Cluster

NOTE: This isn't yet fleshed out as to how it works!

Solr has a concept of node xref:deployment-guide:node-roles.adoc#roles[Roles] that can be leveraged to establish a set of Solr nodes that run embedded ZooKeeper, and then a larger set of Solr nodes that connect to those ZooKeepers. We currently have the concept of "data" nodes that host shards and replicas, and we can introduce a "zookeeper" node that also runs the embedded ZooKeeper process.   

This will work well as you grow from six to 12 nodes in your cluster.

[graphviz]
....
graph simple_embedded_zk_ensemble {

  //size="5,5"
  node [style=rounded]
  layout=circo
  overlap=false
  nodesep=0.3
  ratio=fill;
  node1 [shape=box, label="data, zookeeper", fillcolor=yellow, style="rounded,filled"]
  node2 [shape=box, label="data, zookeeper", fillcolor=yellow, style="rounded,filled"]
  node3 [shape=box, label="data, zookeeper", fillcolor=yellow, style="rounded,filled"]
  node4 [shape=box, label="data"]
  node5 [shape=box, label="data"]
  node6 [shape=box, label="data"]
  node7 [shape=box, label="data"]
  node8 [shape=box, label="data"]
  node9 [shape=box, label="data"]
  
  
  node1 -- node2
  node2 -- node3
  node3 -- node1
  node3 -- node4
  node4 -- node5
  node5 -- node6
  node6 -- node7
  node7 -- node8
  node8 -- node9
  node9 -- node1
}
....

=== Separating out ZooKeeper workload

As your load in the cluster goes up, sharing ZooKeeper workloads with Solr workloads may become a bottleneck.

NOTE: I wonder if this ever goes away by just having Solr nodes with the role `zookeeper` only?

[graphviz]
....
graph dedicate_zk_ensemble {
  node [style=rounded]
  layout=osage
  overlap=false
  node1 [shape=box, label=" zookeeper", fillcolor=yellow, style="rounded,filled"]
  node2 [shape=box, label=" zookeeper", fillcolor=yellow, style="rounded,filled"]
  node3 [shape=box, label=" zookeeper", fillcolor=yellow, style="rounded,filled"]
  node4 [shape=box]
  node5 [shape=box]
  node6 [shape=box]
  node7 [shape=box] 
  node8 [shape=box]
  node9 [shape=box]
  node10 [shape=box]
  node11 [shape=box]
  node12 [shape=box]
  node13 [shape=box] 
  node14 [shape=box]
  node15 [shape=box]
  node16 [shape=box]
  node17 [shape=box]
  node18 [shape=box]
  node19 [shape=box]
  node20 [shape=box]
  
}
....

Use this approach when:

* You go beyond 12 Solr nodes up to 25 Solr nodes.
* You are leveraging all the features of SolrCloud to support multiple collections and different types of query and load characteristics, especially tuning shard and replica counts.
* You may need to move to five ZooKeepers.

Some cons to this approach are:

* You are responsible for configuring and maintaining the external ZooKeeper ensemble.
* You need to define how you will handle failover/HA for the ZooKeeper ensemble itself.

=== Going massive means going Kubernetes

Beyond 25 nodes, you really need to think about more advanced tooling for managing all your nodes. 

[graphviz]
....
graph kubernetes_setup {
  fontname="Helvetica,Arial,sans-serif"
  node [fontname="Helvetica,Arial,sans-serif"]
  edge [fontname="Helvetica,Arial,sans-serif"]
  layout=fdp
  pack=1
  
  "Solr Operator" [fillcolor=aqua, style="filled"]
  
  zk1 [shape=box, label=" zookeeper", fillcolor=yellow, style="rounded,filled"]
  zk2 [shape=box, label=" zookeeper", fillcolor=yellow, style="rounded,filled"]
  zk3 [shape=box, label=" zookeeper", fillcolor=yellow, style="rounded,filled"]
  
  subgraph clusterKubernetes {
      
    "Solr Operator";
    subgraph clusterSolr {
      node1
      node2
      node3
      node4
      node5
      node6
      node7
      node8
      node9
      node10
      node11
      node12
      node13
      node14
      node15
      node16
      node17
      node18
      node19
      node20
      node21
      node22
      node23
      node24
      node25
      node26
      node27
      node28
      node29
      node30
     
    }
    subgraph clusterZK {
      zk1 -- zk2;
      zk2 -- zk3;
      zk3 -- zk1;
    }
  }

  clusterSolr -- clusterZK
}
....

Use this approach when:

* You need to deploy more than 25 Solr nodes.
* You have the operational maturity to manage massive data sets.
* You want a standardized approach to deployment, scaling, and management.
* You may adopt this earlier if you are already a Kubernetes-savvy organization.

Some con's to this approach are:

* Kubernetes has a steep learning curve; it's advisable to have experienced team members or consultants.
* Managing stateful applications like Solr in Kubernetes requires careful planning for persistence and recovery.

== What about User Managed Solr?

The User Managed mode is no longer recommended. Historically, it was primarily used because running embedded ZooKeeper was viewed as difficult.  
These days, running embedded ZooKeeper is straightforward, eliminating the main reason for User Managed deployments. Additionally, User Managed mode doesn't support all the features and APIs that SolrCloud provides.

== What about Embedding Solr in my Java Application?

{solr-javadocs}/core/org/apache/solr/client/solrj/embedded/EmbeddedSolrServer.html[Embedded Solr] is used extensively in Solr's own unit testing strategy.  
It's also frequently used to build dedicated indexes in distributed systems like Spark.  
YMMV.  

== What about [YOUR SPECIFIC NEED]

There are Solr use cases that require extreme scaling on certain specific axes, whether that is a massive multi-tenant use case, extreme query load, or extreme ingestion performance.

Each of these requirements will bring its own specific best practices that you will need to embrace, and have their own impact on how you deploy Solr.

Learn more on xref:optimize-extreme-use-cases.adoc[Optimizing for Extreme Use Cases] page.
